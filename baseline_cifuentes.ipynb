{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    }
   },
   "source": [
    "# Baseline tarea 1\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Importar librerías y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:52.624502Z",
     "start_time": "2019-08-21T19:45:48.613907Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             cohen_kappa_score,\n",
    "                             classification_report,\n",
    "                             accuracy_score,\n",
    "                             roc_auc_score,\n",
    "                             make_scorer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import auc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setear semillas\n",
    "SEED = 8080\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "### Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.819485Z",
     "start_time": "2019-08-21T19:45:52.626520Z"
    }
   },
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data'\n",
    "col_names = ['id', 'tweet', 'class', 'sentiment_intensity']\n",
    "sentiments = ['anger', 'fear', 'joy', 'sadness']\n",
    "split_names = ['train', 'target']\n",
    "train, target = [\n",
    "    {\n",
    "        sentiment : pd.read_csv(\n",
    "            f\"{base_url}/{split_name}/{sentiment}-{split_name}.txt\",\n",
    "            sep='\\t',\n",
    "            names=col_names\n",
    "        )\n",
    "        for sentiment in sentiments\n",
    "    }\n",
    "    for split_name in split_names\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.879257Z",
     "start_time": "2019-08-21T19:45:55.826364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count())\n",
    "\n",
    "\n",
    "for key in train:\n",
    "    get_group_dist(key, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_sampling(train, rs):\n",
    "    new_train = copy.deepcopy(train)\n",
    "    intensities = ['high', 'medium', 'low']\n",
    "    for key in new_train:\n",
    "        cants = np.array([new_train[key]['id'][new_train[key].sentiment_intensity == tens].count()\n",
    "                          for tens in intensities])\n",
    "        max_who = intensities[list(cants).index(cants.max())]\n",
    "        new_max = int((cants.mean() - cants.max()/3) * 3/2)\n",
    "        # Aqui estan los reducidos reducidos\n",
    "        max_sample = new_train[key][new_train[key].sentiment_intensity == max_who].sample(new_max, random_state = rs)\n",
    "        the_rest = new_train[key][new_train[key].sentiment_intensity != max_who].copy()\n",
    "        all_data = the_rest.append(max_sample, ignore_index = True)\n",
    "        new_train[key] = all_data\n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               162    162    162\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               279    279    279\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               207    207    207\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               203    203    203\n"
     ]
    }
   ],
   "source": [
    "new_train = fair_sampling(train, 8080)\n",
    "for key in new_train:\n",
    "    get_group_dist(key, new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el dataset en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.901161Z",
     "start_time": "2019-08-21T19:45:55.893181Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    # Dividir el dataset en train set y test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33,\n",
    "        random_state=8080,\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir el clasificador\n",
    "\n",
    "Consejo para el vectorizador: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation`. También, el parámetro ngram_range para clasificadores no bayesianos.\n",
    "\n",
    "Consejo para el clasificador: investigar otros clasificadores mas efectivos que naive bayes. Ojo q naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armando un embedding vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, wordvector, tokenize):\n",
    "        self.word2vec = wordvector\n",
    "        self.tokenize = tokenize\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = wordvector.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[word] for word in self.tokenize(phrase) if word in self.word2vec.index2word]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for phrase in X.array\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastiancif/Documents/Ramos/NLP/nlpenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wordvector = model_glove_twitter.wv\n",
    "token = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "mev = MeanEmbeddingVectorizer(wordvector, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:50:52.114345Z",
     "start_time": "2019-08-21T19:50:52.110384Z"
    }
   },
   "outputs": [],
   "source": [
    "mytokenizer = TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)\n",
    "vectorizer = CountVectorizer(tokenizer=mytokenizer.tokenize, ngram_range=(1, 1))\n",
    "\n",
    "\n",
    "def get_bagging(base = SVC(kernel = 'linear', probability = True), n_est = 10, vect = vectorizer):\n",
    "    # Inicializamos el Clasificador.\n",
    "    classifier = BaggingClassifier(base_estimator = base,\n",
    "                                   n_estimators = n_est,\n",
    "                                   n_jobs = -1)\n",
    "    \n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vect), ('clf', classifier)])\n",
    "    return text_clf\n",
    "\n",
    "\n",
    "def get_ada(base = SVC(kernel = 'linear', probability = True), n_est = 50, vect = vectorizer):    \n",
    "    # Inicializamos el Clasificador.\n",
    "    classifier = AdaBoostClassifier(base_estimator = base, n_estimators = n_est)\n",
    "    \n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vect), ('clf', classifier)])\n",
    "    return text_clf\n",
    "\n",
    "\n",
    "def get_svm_rbf(vect = vectorizer):\n",
    "    # Inicializamos el Clasificador.\n",
    "    classifier = SVC(kernel = 'rbf',\n",
    "                     gamma = 'scale',\n",
    "                     C = 100,\n",
    "                     probability = True)\n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vect), ('clf', classifier)])\n",
    "    return text_clf\n",
    "\n",
    "\n",
    "def get_svm_linear(vect = vectorizer):\n",
    "    # Inicializamos el Clasificador.\n",
    "    classifier = SVC(kernel = 'linear',\n",
    "                     probability = True)\n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vect), ('clf', classifier)])\n",
    "    return text_clf\n",
    "\n",
    "\n",
    "def get_baseline(vect = vectorizer):\n",
    "    # Inicializamos el Clasificador.\n",
    "    classifier = MultinomialNB()\n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vect), ('clf', classifier)])\n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir evaluación\n",
    "\n",
    "Esta función imprime la matriz de confusión, el reporte de clasificación y las metricas usadas en la competencia:\n",
    "\n",
    "\n",
    "- `auc`\n",
    "- `kappa`\n",
    "- `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:24.009626Z",
     "start_time": "2019-08-21T19:53:24.002646Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaulate(predicted, y_test, labels):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador. \n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [labels[np.argmax(item)] for item in predicted]\n",
    "    '''\n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix for {}:\\n'.format(key))\n",
    "\n",
    "    # Classification Report\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    '''\n",
    "    # AUC\n",
    "    print(\"auc: \", auc(y_test, predicted))\n",
    "\n",
    "    # Kappa\n",
    "    print(\"kappa:\", cohen_kappa_score(y_test, predicted_labels))\n",
    "\n",
    "    # Accuracy\n",
    "    print(\"accuracy:\", accuracy_score(y_test, predicted_labels), \"\\n\")\n",
    "\n",
    "    print('------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el clasificador para cierto dataset\n",
    "\n",
    "Clasifica un dataset. Retorna el modelo ya entrenado mas sus labels asociadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:25.978116Z",
     "start_time": "2019-08-21T19:53:25.973129Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify(text_clf ,dataset, key, get_best = False):\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset)\n",
    "    skf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 10)\n",
    "    print(\"Empieza Cross Validation\")\n",
    "    results = cross_validate(text_clf, X_train, y_train, cv = skf,\n",
    "                           scoring = make_scorer(cohen_kappa_score),\n",
    "                           return_estimator = True,\n",
    "                           error_score='raise',\n",
    "                           n_jobs = 3)\n",
    "    print(f\"Resultados CV: Cohen-Kappa {results['test_score'].mean()} +/- {results['test_score'].std()**2}\")          \n",
    "    # Entrenar el clasificador\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted = text_clf.predict_proba(X_test)\n",
    "\n",
    "    # Obtener las clases aprendidas.\n",
    "    learned_labels = text_clf.classes_\n",
    "\n",
    "    # Evaluar\n",
    "    print(f\"Resultados para {key} usando el modelo {text_clf.steps[1][1]}\")\n",
    "    evaulate(predicted, y_test, learned_labels)\n",
    "    return text_clf, learned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el clasificador por cada dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:27.106461Z",
     "start_time": "2019-08-21T19:53:26.933924Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando con el dataset recortado\n",
      "Empieza Cross Validation\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "\n",
    "linear = get_svm_linear(vect = mev)\n",
    "rbf = get_svm_rbf()\n",
    "ada = get_ada()\n",
    "bagg = get_bagging()\n",
    "\n",
    "models = [linear]\n",
    "\n",
    "# Por cada llave en train ('anger', 'fear', 'joy', 'sadness')\n",
    "'''\n",
    "print(\"Probando con el dataset completo\")\n",
    "for model in models3[:1]:\n",
    "    for key in train:\n",
    "        classifier, learned_labels = classify(model, train[key], key, get_best = True)\n",
    "        classifiers.append(classifier)\n",
    "        learned_labels_array.append(learned_labels)\n",
    "'''\n",
    "print(\"Probando con el dataset recortado\")\n",
    "for model in models:\n",
    "    for key in new_train:\n",
    "        classifier, learned_labels = classify(model, new_train[key], key, get_best = True)\n",
    "        classifiers.append(classifier)\n",
    "        learned_labels_array.append(learned_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    }
   },
   "source": [
    "## Predecir target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:50:59.474909Z",
     "start_time": "2019-08-21T19:50:59.469921Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar la predicción y guardar archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T20:00:10.724762Z",
     "start_time": "2019-08-21T20:00:10.576665Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "if (not os.path.isdir('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecir el target set\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones\n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
