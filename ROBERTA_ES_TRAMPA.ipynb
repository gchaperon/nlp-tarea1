{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion\n",
    "La task de esta tarea corresponde a clasificar la intesidad de sentimientos en tweets para cuatro sentimientos distintos, anger, fear, joy y sandness.\n",
    "\n",
    "El dataset de la task consiste en cuatro tablas, una para cada sentimiento, en donde en cada una hay tweets que presentan el sentimiento correspondiente, junto con una etiqueta de la intensidad (high, medium y low) de dicho sentimiento.\n",
    "\n",
    "Para esta tarea se exploran distintas formas de representar los tweets (BoW, TfIdf, Embeddings, etc) y se combinan con distintos clasificadores (Naive Bayes y SVM) para realizar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo relacionado\n",
    "\n",
    "Este trabajo se basa levemente sobre la publicacion \"Emotion Intensities in Tweet\" por Mohammad et al.\n",
    "\n",
    "La tarea de clasificar sentimientos en tweets ha sido extensamente estudiada, pero determinar la intensidad de dichos sentimientos no ha sido explorado lo suficiente. Esto se debe a que hasta antes del trabajo mencionado en el paper no existia un dataset de tweets etiquetados por la intensidad de los sentimientos presentes.\n",
    "\n",
    "En la publicacion se trato de asignarle un nivel de intensidad de sentimiento a los tweets en una escala continua entre 0 y 1, es decir, corresponde a una task de regresion, pero la task que se busca resolver en este trabajo corresponde a una de clasificacion, que es una version simplificada de la original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos y representaciones\n",
    "Para realizar los experimentos se utilizaron distintas tecnicas para representar datos y para realizar las clasificaciones.\n",
    "\n",
    "Nuestros experimentos los basamos en el concepto de Pipelines de scikitlearn, en donde se define una lista de operaciones que se realizaran sobre los datos y luego se ejecutan todas las trasformaciones y reducciones correspondientes.\n",
    "\n",
    "En este contexto, describiremos las distintas etapas del pipeline y explicaremos los distintos metodos que utilizamos en cada una de las etapas. Finalmente se generaron pipelines con combinaciones de las distintas tecnicas mencionadas en cada etapa.\n",
    "\n",
    "### Preproceso\n",
    "Para el preproceso utilizamos varias tecnicas, algunas mas estandar que otras. Probamos:\n",
    "* Removiendo *stop words*\n",
    "* Sacando caracteres especiales, como puntuacion no deseada\n",
    "* Sacando saltos de linea\n",
    "* Removiendo secuencias de escape html\n",
    "\n",
    "Ninguna de estas tecnicas mostro una mejora substancial por si sola.\n",
    "\n",
    "### Tokenizacion\n",
    "Para el proceso de tokenizacion se utilizo la clase *TweetTokenizer* de nltk y se probaron distintas combinaciones de argumentos para realizar un filtrado de los tokens obtenidos. En este ambito se probo:\n",
    "* Remover handles\n",
    "* Transformar a minuscula los tokens\n",
    "* Remover caracteres consecutivos repetidos. Esto significa que cada vez que aparecen 3 o mas caracteres identicos repetidos, se reemplazan por tan solo 3 caracteres consecutivos.\n",
    "* Remover letras aisladas.\n",
    "\n",
    "En general estas tecnicas mejoraron el desempeño, ya que lograban efectivamente reducir la dimensionalidad de los tweets. A pesar de esto las mejoras no fueron estadisticamente significativas.\n",
    "\n",
    "### Vectorizacion\n",
    "Para este paso del pipeline se utilizaron algunas de las tecnicas vistas en clases que supusimos iban a tener buen impacto en las metricas de evaluacion:\n",
    "* Bag of Words\n",
    "* Tfidf\n",
    "* Word n\\*grams\n",
    "* Character n\\*grams\n",
    "* Filtro de aparicion por minima frecuencia\n",
    "\n",
    "La mayoria de estos metodos no cambiaron sustancialmente los resultados, a excepcion del ultimo punto. Esto corresponde a un parametro de los vectorizadores de scikit-learn en donde solo las palabras que aparecen en una cantidad minima de documentos se dejan en la vectorizacion. Esto sumado a la remocion de las *stop words* deja una vectorizacion de alrededor de 200 features y esto mejoro el desempeño de los modelos levemente.\n",
    "\n",
    "### Clasificacion\n",
    "Para la clasificacion se uso como baseline el modelo Naive Bayes Multinomial y se probo usando Suport Vector Machine con distintos kernels y distintos valores de penalizacion. Esta decision se tomo porque en la literatura se menciona que las SVM tienen desempeños buenos en tareas de NLP, y son sensibles alos cambios de los hiperparametros del modelo.\n",
    "\n",
    "Para SVM se exploraron los siguientes hiperparametros:\n",
    "* Penalizacion\n",
    "* Tipo de kernel\n",
    "* Valor de gamma\n",
    "\n",
    "Con lo anterior, al explorar distintos valores para la penalizacion se pudo determinar que el seteo de esta variable era significativo para el rendimiento del clasificador. Asi mismo, la estrategia de calculo del valor gamma tambien influyo en "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
